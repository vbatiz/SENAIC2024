{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY-da-8CdwYe"
      },
      "source": [
        "# Aplicando Transformers en Análisis de Sentimientos.\n",
        "\n",
        "**Investigadores**: <br>\n",
        "  Dr. Ramón Zatarain Cabada<br>\n",
        "  Dra. María Lucía Barrón Estrada<br>\n",
        "  M.C. Víctor Manuel Bátiz Beltrán\n",
        "\n",
        "**Corpus**: SentiText\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRLUAzCwd_ev"
      },
      "source": [
        "### Descripción general\n",
        "Usaremos el dataset SentiText.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGhfC_KdeScW"
      },
      "source": [
        "### Pasos iniciales\n",
        "Intslamos e importamos las bibliotecas a utilizar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7dVKY0QO8vf"
      },
      "outputs": [],
      "source": [
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlDpUQiMdv-H"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "#import matplotlib.pyplot as plt\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from nltk import SnowballStemmer\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "from gensim.utils import simple_preprocess\n",
        "import gensim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import spacy\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "#import seaborn as sns\n",
        "#from sklearn.metrics import confusion_matrix\n",
        "#import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import emoji\n",
        "import keras\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "print('Listo')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3UTXK-VesKm"
      },
      "source": [
        "## 1. Cargando el dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SAXN0fyklKz"
      },
      "source": [
        "### Descargando el corpus desde el sitio Web de PersonApp.\n",
        "\n",
        "La primera celda de código fue necesaria para poder usar el mode GPU, ya que sin ello marcaba error de encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DR10Mvdm9Icr"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUIxJKTQNKQO"
      },
      "outputs": [],
      "source": [
        "def corpus_download(path, url):\n",
        "  !wget --no-check-certificate \\\n",
        "     {url} \\\n",
        "     -O {path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbdzcwUWWRKY"
      },
      "outputs": [],
      "source": [
        "corpus_download(\"SentiText.csv\",\"https://person-app-itc.web.app/corpus/SentiText.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsH0ZtYVV4X5"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"SentiText.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUCCv6KHgWF5"
      },
      "source": [
        "### Exploración de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6jVt8ywgge9"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5dURs-aglsQ"
      },
      "outputs": [],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GnuTqVVNxGw"
      },
      "source": [
        "We change labels to numerical representation 0 = negative and 1= positive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoywSaaaNza_"
      },
      "outputs": [],
      "source": [
        "data['Label'] = data['Label'].replace({'negativo':0, 'positivo':1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVHZt7bXh1TV"
      },
      "source": [
        "## 2. Limpieza de datos (Data cleaning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-8kSnPiilJz"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRd5iPwrD0NI"
      },
      "outputs": [],
      "source": [
        "print(data.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBEs72ot3thj"
      },
      "outputs": [],
      "source": [
        "#Check if we have null fields\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7f7rDx33thl"
      },
      "outputs": [],
      "source": [
        "#In case we have null texts.\n",
        "data[\"Text\"].fillna(\"Sin texto\", inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A86_STi45KO-"
      },
      "source": [
        "### A continuación realizaremos los siguientes pasos:\n",
        "\n",
        "1. Separar el texto en Tokens\n",
        "2. Convertir palabras a minúsculas\n",
        "3. Expandir contracciones\n",
        "4. Remover urls, correos, saltos de línea\n",
        "5. Eliminar caracteres repetidos\n",
        "6. Eliminar nuevas líneas y pestañas\n",
        "7. Remover saltos de línea\n",
        "8. Remover comillas simples\n",
        "9. Eliminar comas \" , \"\n",
        "10. Remover números\n",
        "11. Remover Caracteres no alfanuméricos\n",
        "12. Eliminar guiones entre palabras\n",
        "13. Eliminar los guiones dobles y triples\n",
        "14. Eliminar espacios en blanco (al principio, final y espacios dobles)\n",
        "15. Eleminar stop words\n",
        "16. Realizar stemming/Lematizacion  \n",
        "17. Remover signos de puntuación\n",
        "18. Destokenizar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiCQNY1ZkZur"
      },
      "outputs": [],
      "source": [
        "def depurar_datos(data):\n",
        "\n",
        "    #Remover URLs\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    data = url_pattern.sub(r'', data)\n",
        "\n",
        "    # Removee correos\n",
        "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
        "\n",
        "    # Remover saltos de línea\n",
        "    data = re.sub('\\s+', ' ', data)\n",
        "\n",
        "    #Convertir palabras a minúscula\n",
        "    data=data.lower()\n",
        "\n",
        "    # Remover comillas simples\n",
        "    data = re.sub(\"\\'\", \"\", data)\n",
        "\n",
        "    # Remover numeros\n",
        "    data = re.sub(r'\\d+', '', data)\n",
        "\n",
        "    #Remover Caracteres especiales y numeros\n",
        "    data = re.sub(r\"[^a-zA-Z-á-é,í,ó,ú,ü,Á-É-Í-Ó-Ú-ñ]\",\" \",data)\n",
        "\n",
        "\n",
        "    #Eliminar los espacios en blanco al principio\n",
        "    data= re.sub(r\"^\\s+\", \"\", data)\n",
        "\n",
        "    #Eliminar los espacios en blanco al final\n",
        "    data= re.sub(r\"\\s+$\", \"\", data)\n",
        "\n",
        "    #remover espacios dobles\n",
        "    data = \" \".join(data.split())\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VYc3axxOOrv"
      },
      "outputs": [],
      "source": [
        "def process_text0(text):\n",
        "    regex = r'https://\\S+|\\B@\\w+\\b'\n",
        "    text = re.sub(regex, '', text)\n",
        "    text = re.sub(r'([\\U0001F300-\\U0001F64F\\U0001F680-\\U0001F6FF])', r'\\1 ', text)\n",
        "    text = emoji.demojize(text)\n",
        "    text = text.replace(\"  \", \" \")\n",
        "    return text\n",
        "\n",
        "def process_text(sentence, norm_user = True, norm_hashtag = True, separate_characters = True):\n",
        "    # Convert instance to string\n",
        "    sentence = str(sentence)\n",
        "\n",
        "    # All text to lowecase\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # Normalize users and url\n",
        "    if norm_user == True:\n",
        "        sentence = re.sub(r'\\@\\w+','@usuario', sentence)\n",
        "    if norm_hashtag == True:\n",
        "        sentence = re.sub(r\"http\\S+|www\\S+|https\\S+\", 'url', sentence, flags=re.MULTILINE)\n",
        "\n",
        "    # Separate special characters\n",
        "    if separate_characters == True:\n",
        "        sentence = re.sub(r\":\", \" : \", sentence)\n",
        "        sentence = re.sub(r\",\", \" , \", sentence)\n",
        "        sentence = re.sub(r\"\\.\", \" . \", sentence)\n",
        "        sentence = re.sub(r\"!\", \" ! \", sentence)\n",
        "        sentence = re.sub(r\"¡\", \" ¡ \", sentence)\n",
        "        sentence = re.sub(r\"“\", \" “ \", sentence)\n",
        "        sentence = re.sub(r\"'\", \" ' \", sentence)\n",
        "        sentence = re.sub(r\"”\", \" ” \", sentence)\n",
        "        sentence = re.sub(r\"\\(\", \" ( \", sentence)\n",
        "        sentence = re.sub(r\"\\)\", \" ) \", sentence)\n",
        "        sentence = re.sub(r\"\\?\", \" ? \", sentence)\n",
        "        sentence = re.sub(r\"\\¿\", \" ¿ \", sentence)\n",
        "\n",
        "    # Substituting multiple spaces with single space\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence, flags=re.I)\n",
        "    # emojis to text\n",
        "    sentence = emoji.demojize(sentence)\n",
        "\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fASUMMIRO0h"
      },
      "outputs": [],
      "source": [
        "clean_data = data.copy()\n",
        "clean_data['Text'] = clean_data['Text'].apply(process_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bToz0Lw2RUoW"
      },
      "outputs": [],
      "source": [
        "clean_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhdcW-9yipkV"
      },
      "source": [
        "### Eliminando las palabras que no aportan valor (stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx3MC1ySR2Hw"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgDFlsngR_qu"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "print(stopwords.words('spanish'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcYfHBRKSl96"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('spanish'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_22ocDzPipkW"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  no_stopwords = [word for word in word_tokens if not word in stop_words]\n",
        "  return \" \".join(no_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5_q4pn2TgWG"
      },
      "outputs": [],
      "source": [
        "remove_stopwords('el que tiene tienda la debe atender')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AWGP49HVHpn"
      },
      "outputs": [],
      "source": [
        "clean_data['Text'] = clean_data['Text'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWiHifnQEsVz"
      },
      "source": [
        "### Lematización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eyJX4otEsV4"
      },
      "outputs": [],
      "source": [
        "#https://spacy.io/models/es\n",
        "#We'll use Spacy for Lematization\n",
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GD_GPGQUEsV6"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import es_core_news_sm\n",
        "nlp = es_core_news_sm.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWZh2H0_Ws-e"
      },
      "outputs": [],
      "source": [
        "def lematize(text):\n",
        "    doc = nlp(text)\n",
        "    lemms = []\n",
        "    for token in doc:\n",
        "        lemms.append(token.lemma_)\n",
        "    return \" \".join(lemms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mLXZM0qW3Dn"
      },
      "outputs": [],
      "source": [
        "lematize('yo soy muy feliz con mi familia')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRcBnou-XBxV"
      },
      "outputs": [],
      "source": [
        "clean_data['Text'] = clean_data['Text'].apply(lematize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4X5sRyNPEF_"
      },
      "source": [
        "### Retirando elementos de puntuación y acentos (Punctuation Cleaning)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfzOCwJMYC3_"
      },
      "outputs": [],
      "source": [
        "def cleaning_punct(text):\n",
        "  token_list = gensim.utils.simple_preprocess(str(text), deacc=True)  # deacc=True remueve puntuación\n",
        "  return \" \".join(token_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCyNs1G4YPYn"
      },
      "outputs": [],
      "source": [
        "cleaning_punct('mi méxico querido qué fantástico')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLAMu8jjYuIw"
      },
      "outputs": [],
      "source": [
        "clean_data['Text'] = clean_data['Text'].apply(cleaning_punct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0jMQ8sspP75"
      },
      "source": [
        "## 3. Construcción del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Jcxo_8gwhH0"
      },
      "outputs": [],
      "source": [
        "#clases = ['Negative','Positive']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS6B50N_UNvN"
      },
      "source": [
        "### Transformers\n",
        "\n",
        "Pasos iniciales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNHdADgrn979"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.24.0\n",
        "!pip install simpletransformers==0.63.11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPHYDE9B10Ei"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install simpletransformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paPlX2MjUVXl"
      },
      "outputs": [],
      "source": [
        "!pip install emoji\n",
        "# install simpletransformers\n",
        "#!pip install simpletransformers\n",
        "\n",
        "# check installed version\n",
        "#!pip freeze | grep simpletransformers\n",
        "# simpletransformers==0.28.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNkYZhEAnX6f"
      },
      "outputs": [],
      "source": [
        "pip show simpletransformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcsqD5ehVPAm"
      },
      "source": [
        "### Cargando los modelos preentrenados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYAFeYXSVVtm"
      },
      "outputs": [],
      "source": [
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging # Import the logging module\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)"
      ],
      "metadata": {
        "id": "ytRxN0uJQsQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPJwQIaVV6Bm"
      },
      "outputs": [],
      "source": [
        "#logging.basicConfig(level=logging.INFO)\n",
        "#transformers_logger = logging.getLogger(\"transformers\")\n",
        "#transformers_logger.setLevel(logging.WARNING)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GztXi0XEcbUk"
      },
      "outputs": [],
      "source": [
        "clean_data2 = clean_data.copy()\n",
        "clean_data2.rename(columns = {'Text':'text','Label':'labels'}, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZ8q5orLbeV0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(clean_data2, test_size=0.20)\n",
        "\n",
        "print('train shape: ',train_df.shape)\n",
        "print('test shape: ',test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s75eAZFyV9cV"
      },
      "outputs": [],
      "source": [
        "# Optional model configuration\n",
        "model_args = ClassificationArgs(num_train_epochs=1)\n",
        "\n",
        "train_args ={\"reprocess_input_data\": True,\n",
        "             \"fp16\":False,\n",
        "             \"num_train_epochs\": 1, # Usaremos una época por cuestiones de tiempo\n",
        "             \"overwrite_output_dir\": True}\n",
        "\n",
        "# Create a ClassificationModel\n",
        "model = ClassificationModel(\n",
        "    'bert',\n",
        "    'bert-base-uncased',\n",
        "    num_labels=2,\n",
        "    args=train_args\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamos el modelo"
      ],
      "metadata": {
        "id": "HDjKiaRIZZp4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWxsSxOMW5qG"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model.train_model(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NucAtxdmXls9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbUYu240W89t"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "result, model_outputs, wrong_predictions = model.eval_model(test_df,f1=f1_score, acc=accuracy_score, rc=recall_score, pcs=precision_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnthKXTLYBY1"
      },
      "outputs": [],
      "source": [
        "result['acc']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-2Fob8SYLaO"
      },
      "source": [
        "### Probando el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEofu7WuYO5E"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import recall_score\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcytEnypvyDp"
      },
      "outputs": [],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcrK7b7IYUP3"
      },
      "outputs": [],
      "source": [
        "test = test_df['text'].to_numpy().tolist()\n",
        "y = test_df['labels'].to_numpy().tolist()\n",
        "print(test[0:10])\n",
        "print(y[0:10])\n",
        "print(len(test))\n",
        "print(len(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m56_jD-BYniM"
      },
      "outputs": [],
      "source": [
        "predictions_test = model.predict(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOgNB0IeY-6O"
      },
      "outputs": [],
      "source": [
        "test_recall = metrics.recall_score(y, predictions_test[0], average='macro')\n",
        "test_f1 = metrics.f1_score(y, predictions_test[0], average='macro')\n",
        "test_precision = metrics.precision_score(y, predictions_test[0], average='macro')\n",
        "test_accuracy = metrics.accuracy_score(y, predictions_test[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3oHLtUIZIM2"
      },
      "outputs": [],
      "source": [
        "print(\"Metrics results:\")\n",
        "print(f\"Accuracy: {test_accuracy}\")\n",
        "print(f\"F1: {test_f1}\")\n",
        "print(f\"Precision: {test_precision}\")\n",
        "print(f\"Recall: {test_recall}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "DRLUAzCwd_ev",
        "1Ep-76Jtqwq3",
        "lX0BijfnulTM",
        "Fg2s29AS24wd"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}