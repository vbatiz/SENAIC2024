{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Herramientas generales para la generación del corpus.\n",
    "\n",
    "**Investigadores**: <br>\n",
    "  Dr. Ramón Zatarain Cabada<br>\n",
    "  Dra. María Lucía Barrón Estrada<br>\n",
    "  M.C. Víctor Manuel Bátiz Beltrán"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de Bibliotecas\n",
    "\n",
    "Los siguientes comandos instalarán los paquetes de Python necesarios para grabar fragmentos de audio y utilizar los modelos Whisper para la transcripción de voz a texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/openai/whisper.git\n",
    "! pip install sounddevice wavio\n",
    "! pip install ipywebrtc notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También ocuparemos lo siguiente para poder grabar audio desde este cuaderno y poder procesar los archivos resultantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!apt install ffmpeg\n",
    "!apt-get install libportaudio2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import whisper\n",
    "import torchaudio\n",
    "\n",
    "from ipywebrtc import AudioRecorder, CameraStream\n",
    "from IPython.display import Audio, display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizamos una grabación de prueba\n",
    "\n",
    "Activaremos algunos widgets que nos proporciona Colab para poder realizar la grabación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulsa el botón circular y empieza a hablar. Puede que no lo parezca, pero el widget estará capturando sonido. Pulsa de nuevo el botón del círculo cuando hayas terminado. El widget empezará inmediatamente a reproducir lo que ha capturado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "camera = CameraStream(constraints={'audio': True,'video':False})\n",
    "recorder = AudioRecorder(stream=camera)\n",
    "recorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El formato de audio capturado anteriormente no es legible por PyTorch. En este paso, convertimos nuestra grabación a un formato que PyTorch pueda entender. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with open('recording.webm', 'wb') as f:\n",
    "    f.write(recorder.audio.value)\n",
    "!ffmpeg -i recording.webm -ac 1 -f wav my_recording.wav -y -hide_banner -loglevel panic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opciones de selección\n",
    "\n",
    "Whisper es capaz de realizar transcripciones para muchos idiomas (aunque funciona mejor para algunos idiomas y peor para otros). \n",
    "\n",
    "Whisper también es capaz de detectar el idioma de entrada. Sin embargo, para estar seguros, podemos decirle explícitamente a Whisper qué idioma debe esperar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "language_options = whisper.tokenizer.TO_LANGUAGE_CODE \n",
    "language_list = list(language_options.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "lang_dropdown = widgets.Dropdown(options=language_list, value='spanish')\n",
    "output = widgets.Output()\n",
    "display(lang_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whisper también es capaz de realizar varias tareas, como la transcripción sólo en inglés, la traducción de cualquier idioma al inglés y la transcripción de idiomas distintos del inglés. \n",
    "\n",
    "A continuación puede seleccionar «transcripción» (que producirá texto en el mismo idioma que el de entrada) o «traducción» (que transcribirá de no inglés a inglés). \n",
    "\n",
    "![Capacidades de Whisper](https://cdn.openai.com/whisper/draft-20220920a/asr-training-data-desktop.svg)\n",
    "\n",
    "Imagen tomada de [Introducción a Whisper](https://openai.com/blog/whisper/) by OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "task_dropdown = widgets.Dropdown(options=['transcribe', 'translate'], value='transcribe')\n",
    "output = widgets.Output()\n",
    "display(task_dropdown)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
